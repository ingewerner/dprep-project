{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful scraper of team 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! Nice that you want to scrape twitter. Before we go to the hard coding you need to install a few things and import it in python. We also would like to refer to our documentation \"Beautiful_Scraper_of_team_9.pdf\" where you can read everything about our purpose of this scraper and the set up of it. Best, Aithra Spandagou, Marjolein Keijzer and Inge Werner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\ingew\\appdata\\local\\temp\\pip-req-build-_g04dbi1\n",
      "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev96+g47fbc2a from git+https://github.com/JustAnotherArchivist/snscrape.git in c:\\users\\ingew\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ingew\\anaconda\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ingew\\anaconda\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.6.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.9.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\ingew\\anaconda\\lib\\site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.25.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\ingew\\anaconda\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\ingew\\anaconda\\lib\\site-packages (from beautifulsoup4->snscrape==0.3.5.dev96+g47fbc2a) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py): started\n",
      "  Building wheel for snscrape (setup.py): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.3.5.dev96+g47fbc2a-py3-none-any.whl size=50496 sha256=56313c20d69a62467ed366e3137e810db7c00c0be12189421f1a6b22485010df\n",
      "  Stored in directory: C:\\Users\\ingew\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-zrcmh95q\\wheels\\92\\42\\87\\33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Requirement already satisfied: pandas in c:\\users\\ingew\\anaconda\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ingew\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#Run the pip install command below if you don't already have the library\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "#Run the below command if you don't already have Pandas\n",
    "!pip install pandas\n",
    "\n",
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yeah good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the preparation. We will make a function now (get_twitterdata). We will include entities which we will use for our own research, however you can extend or adjust it with other entities you might like. You can find other entities at: betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af .\n",
    "We also make use of sntwitter.TwitterSearchScraper, a package which makes it very easo to retrieve historical data of Twitter. The downside of this package is that it is a package made by just \"someone\" so it is unsure you can still use it in 10 years for example. However, two articles are made for it on two different websites . These websites, betterprogramming.pub and medium.com, have a lot of fans so we are assuming that whenever the code does not work there will be a solution and these websites will publish it. Also, the GitHub pages with the snscrape package (the websites are reffering to) are 30 times forked and \"starred\" over 30 times. This makes the package a bit more stable since people are actually working with the package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitterdata(searchq, maxTweets, filename, since, until, lang): \n",
    "    timefilter= 'since:' + since +' until:'+until\n",
    "    print (\"Hi! I'm starting up for time period: \" + timefilter)\n",
    "    \n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list1 = [] \n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchq + ' ' + timefilter + ' ' + lang).get_items()):\n",
    "        if i>maxTweets: #stops when amount of tweets is at its maximum\n",
    "            break\n",
    "        tweets_list1.append([tweet.date, tweet.id, tweet.content.replace('\\t', ' '), tweet.user.id, tweet.user.location.replace('\\t', ' '), \n",
    "                             tweet.retweetCount, tweet.likeCount, tweet.replyCount, tweet.lang, tweet.url, tweet.renderedContent.replace('\\t', ' '), \n",
    "                             tweet.user.description.replace('\\t', ' '), tweet.user.created, tweet.user.followersCount, tweet.user.friendsCount, \n",
    "                             tweet.user.statusesCount, tweet.user.listedCount, tweet.user.mediaCount, tweet.quoteCount, tweet.source, tweet.retweetedTweet,\n",
    "                             tweet.quotedTweet]) #add all the items we want to subtrect\n",
    "    print('Done! Now saving!')    \n",
    "   \n",
    "    # create dataframe\n",
    "    tweets_df1 = pd.DataFrame(tweets_list1, columns=['datetime', 'tweet_id', 'text', 'user_id', 'location', 'retweet', 'like', 'reply', 'language', 'url',\n",
    "                                                    'rendered_content', 'user_id' 'user_description', 'user_created', 'user_amount_followers', \n",
    "                                                    'user_amount_friends', 'user_amount_status', 'user_listed_count', 'user_media_count', 'quote_count', \n",
    "                                                    'source_tweet', 'retweeted_tweet', 'quoted_tweet']) #give the collumns names and create our Data Frame\n",
    "\n",
    "    # Export dataframe into a CSV\n",
    "    tweets_df1.to_csv(filename, sep='\\t', index=False)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wooow you have set up a function! Let's see how to use it now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the function you need to define the variables of the function. See below. We first want to have a dataset for our baseline which is a period in 2020. Then, we also want to have another dataset with a period in 2021 so we adjust the \"since\" and \"until\". You can do that however with all variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application first dataset (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm starting up for time period: since:2020-01-01 until:2020-03-17\n",
      "Done! Now saving!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_twitterdata(searchq ='#BLM OR #blacklivesmatter',\n",
    "                maxTweets = 10000,\n",
    "                filename = 'BLM2020_Dataset.csv',\n",
    "                since = '2020-01-01',\n",
    "                until = '2020-03-17',\n",
    "                lang = 'lang:nl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now you can see the csv.file in your pc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application second dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm starting up for time period: since:2021-01-01 until:2021-03-17\n",
      "Done! Now saving!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_twitterdata(searchq ='#BLM OR #blacklivesmatter', \n",
    "                maxTweets = 10000, \n",
    "                filename = 'BLM2021_Dataset.csv',\n",
    "                since = '2021-01-01', \n",
    "                until = '2021-03-17',\n",
    "                lang = 'lang:nl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was it! Easy right? Now you have two datasets which you can use for other research or projects. Good luck! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
